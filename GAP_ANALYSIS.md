# Frontier Gap Analysis

| Domain | Metric | Current | Target | Evidence | Risk |
| --- | --- | --- | --- | --- | --- |
| Delivery / DevEx | DORA instrumentation | CI builds, tests, image pushes, and dev deploys execute without collecting deploy frequency, change failure rate, or MTTR telemetry. | Daily deploys with lead time <24h, CFR ≤15%, MTTR <1h. | `.github/workflows/ci.yml` shows build/test/deploy steps but no telemetry export.【F:.github/workflows/ci.yml†L1-L180】 | Cannot quantify delivery performance or detect regressions.
| Delivery / DevEx | Flake tracking | Pytest matrix runs once per commit without retry/flake accounting; failures rely on manual triage. | Automated flake detection and quarantine. | CI workflow lacks flake analysis or rerun hooks.【F:.github/workflows/ci.yml†L45-L55】 | Hidden instability increases MTTR and wastes engineer time.
| Reliability / Ops | Service SLOs & error budgets | Operations guide lists KPI targets but no formal SLO/error-budget definitions or alert thresholds. | Documented SLOs per service with budget burn tracking. | `docs/operations-guide.md` enumerates KPIs without SLO/error-budget process.【F:docs/operations-guide.md†L135-L171】 | Incident response lacks objective triggers; risk of unmanaged burn rate.
| Reliability / Ops | Golden Signals dashboards | Make targets start Prometheus/Grafana, but no curated dashboards or alert wiring exist in repo. | Golden Signal dashboards with alert rules per service. | `Makefile` only provides commands to launch Prometheus/Grafana with no dashboard JSON or alert config.【F:Makefile†L120-L177】 | Operators lack shared situational awareness during incidents.
| Performance (Backend) | Retrieval benchmarking | Performance benchmarking falls back to mocked metrics unless feature flag enables optional evaluator. | Always-on BEIR-style evaluations with stored baselines. | Retrieval benchmark code defaults to synthetic data when evaluator disabled.【F:packages/api/src/stratmaster_api/performance.py†L320-L398】 | False confidence in retrieval quality; regressions can slip.
| Performance (Frontend) | Core Web Vitals | Lighthouse budget tracks legacy metrics (FCP, LCP, CLS, TBT) but omits INP and uses lenient thresholds. | Lab tests enforcing LCP ≤2.5s, INP ≤200ms, CLS ≤0.1. | `lighthouse-budget.json` lacks INP and sets LCP budget to 4s.【F:lighthouse-budget.json†L27-L49】 | UI can regress on INP without CI detection, harming UX.
| Security / Supply Chain | Threat modelling depth | STRIDE document remains a placeholder skeleton without enumerated threats or mitigations. | Comprehensive ASVS L2 + OWASP LLM Top-10 coverage with residual risk tracking. | `ops/threat-model/stride.md` lists only a stub description.【F:ops/threat-model/stride.md†L1-L3】 | Blind spots in threat coverage and audit trails.
| Security / Supply Chain | SBOM generation | CI runs bandit/pip-audit in warn-only mode but does not emit signed SBOMs per artefact. | Automated SBOM per build with SLSA provenance. | `ci.yml` security steps scan dependencies yet produce no SBOM artifacts.【F:.github/workflows/ci.yml†L55-L71】 | Supply-chain visibility gaps complicate compliance reporting.
| Code Quality / Architecture | Mutation & architecture gates | No mutation testing or dependency cycle checks run in CI; parity job marked TODO. | Quality gates enforcing mutation score ≥60% and cycle-free graphs. | Docs workflow parity check contains TODO stubs instead of real enforcement.【F:.github/workflows/docs.yml†L134-L167】 | Critical logic changes may ship without robustness validation.
| AI Orchestration | Model recommender evidence | Model recommender uses cached sample data whenever V2 feature flag disabled; persistence optional. | Persistent ingestion of LMSYS/MTEB/RAGAS data feeding routing decisions. | Model recommender returns static fallback data outside V2 path.【F:packages/mcp-servers/router-mcp/src/router_mcp/model_recommender.py†L117-L195】 | Router choices may drift from real-world performance and cost.
| UX / Accessibility | Regression detection | Accessibility workflow runs Lighthouse/axe but regression comparison step is placeholder logging. | Automated diffing with thresholds tied to HEART/WCAG backlog. | Performance regression job echoes TODO summary instead of diffing reports.【F:.github/workflows/accessibility-quality-gates.yml†L168-L210】 | Regressions in UX metrics may go unnoticed between releases.
| Docs & Versioning | Diátaxis parity | Documentation parity check lacks endpoint extraction/coverage validation scripts. | Automated docs coverage with lychee/doctest gates passing. | Docs workflow leaves TODO markers for parity scripts.【F:.github/workflows/docs.yml†L155-L167】 | Docs can fall behind APIs, undermining onboarding and compliance.

## Key Observations
- Multiple frontier metrics (DORA, SLO burn rates, INP) lack instrumentation; follow-up plan focuses on enabling telemetry before tuning thresholds.
- Security posture requires structured threat modelling and SBOM automation to satisfy ASVS/SLSA commitments.
- Existing workflows provide scaffolding (Lighthouse budgets, Prometheus stack, model recommender) but rely on feature flags or TODOs, indicating partially complete implementations needing finish work.
