# vLLM/Ollama â€” Infra Stub

- Purpose: Local model serving for completions/embeddings; structured decoding.
- TODO[INF-408a]: Catalogue supported models, GPU/CPU footprints, and fallback rules; align with router defaults (`docs/backlog.md#todo-inf-408-vllmollama-serving`).
- TODO[INF-408b]: Provide guided JSON/grammar decoding configuration examples and resource sizing guidance for dev/staging/production.
