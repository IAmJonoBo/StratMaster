version: "3.9"

services:
  api:
    build: ./packages/api
    ports:
      - "8080:8080"
    environment:
      PYTHONUNBUFFERED: "1"
      # checkov:skip=CKV_SECRET_4: Dev-only embedded credentials for local Postgres
      # checkov:skip=CKV_SECRET_6: High-entropy detection false-positive for local-only URL
      STRATMASTER_API_DB_URL: postgresql+psycopg://postgres:postgres@postgres:5432/stratmaster
      LANGFUSE_SERVER_URL: http://langfuse:3000
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      REDIS_URL: redis://redis:6379/0
      # Phase 2 telemetry configuration
      GRAFANA_URL: http://grafana:3000
      PROMETHEUS_URL: http://prometheus:9090
      # Development configuration
      STRATMASTER_LOG_LEVEL: INFO
      STRATMASTER_RELOAD: "true"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    develop:
      watch:
        - path: ./packages/api
          action: sync
          target: /app

  research-mcp:
    build: ./packages/mcp-servers/research-mcp
    ports:
      - "8081:8081"
    environment:
      PYTHONUNBUFFERED: "1"
      RESEARCH_MCP_ALLOWLIST: example.com
      RESEARCH_MCP_CACHE_DIR: /cache
      RESEARCH_MCP_ENABLE_NETWORK: "1"
      SEARXNG_ENDPOINT: http://searxng:8080/search
    volumes:
      - research_cache:/cache
    develop:
      watch:
        - path: ./packages/mcp-servers/research-mcp
          action: sync
          target: /app

  knowledge-mcp:
    build: ./packages/mcp-servers/knowledge-mcp
    ports:
      - "8082:8082"
    environment:
      PYTHONUNBUFFERED: "1"
      KNOWLEDGE_MCP_VECTOR_HOST: http://qdrant:6333
      KNOWLEDGE_MCP_VECTOR_COLLECTION: default
      KNOWLEDGE_MCP_KEYWORD_HOST: http://opensearch:9200
      KNOWLEDGE_MCP_KEYWORD_INDEX: documents
      KNOWLEDGE_MCP_GRAPH_HOST: nebula://nebula:9669
      KNOWLEDGE_MCP_GRAPH_SPACE: knowledge
    develop:
      watch:
        - path: ./packages/mcp-servers/knowledge-mcp
          action: sync
          target: /app

  # LiteLLM Proxy - OSS-first model gateway with cloud fallbacks
  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - "8090:4000"  # LiteLLM default port
    environment:
      PYTHONUNBUFFERED: "1"
      # LiteLLM configuration
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-1234}
      DATABASE_URL: postgresql+psycopg://postgres:postgres@postgres:5432/litellm
      # OpenTelemetry integration 
      OTEL_EXPORTER_OTLP_ENDPOINT: http://otel-collector:4317
      # External API keys for cloud fallbacks
      TOGETHER_API_KEY: ${TOGETHER_API_KEY:-}
      HUGGINGFACE_API_KEY: ${HUGGINGFACE_API_KEY:-}
      # Redis for caching and performance (<20ms routing target)
      REDIS_HOST: redis
      REDIS_PORT: 6379
    volumes:
      - ./configs/litellm-proxy.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # vLLM - Local high-throughput text/vision chat and embeddings
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8091:8000"
    environment:
      PYTHONUNBUFFERED: "1"
    command: [
      "--model", "meta-llama/Llama-3.1-8B-Instruct",
      "--served-model-name", "llama-3.1-8b-instruct",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--max-model-len", "8192",
      "--disable-log-requests"
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # TGI - GPU-efficient generation and re-ranking
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:latest
    ports:
      - "8092:8080"
    environment:
      PYTHONUNBUFFERED: "1"
      MODEL_ID: meta-llama/Meta-Llama-3-70B-Instruct
      MAX_INPUT_LENGTH: "4096"
      MAX_TOTAL_TOKENS: "8192"
      PORT: "8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  router-mcp:
    build: ./packages/mcp-servers/router-mcp
    ports:
      - "8083:8083"
    environment:
      PYTHONUNBUFFERED: "1"
      # Update to use LiteLLM proxy as primary backend
      ROUTER_MCP_PROVIDER: litellm
      ROUTER_MCP_BASE_URL: http://litellm-proxy:4000
      ROUTER_MCP_API_KEY: ${LITELLM_MASTER_KEY:-sk-1234}
      ROUTER_MCP_COMPLETION_MODEL: chat-default
      ROUTER_MCP_EMBEDDING_MODEL: embed-default
      ROUTER_MCP_RERANK_MODEL: bge-reranker-large
      ROUTER_MCP_TEMPERATURE: "0.2"
    depends_on:
      litellm-proxy:
        condition: service_healthy
    develop:
      watch:
        - path: ./packages/mcp-servers/router-mcp
          action: sync
          target: /app

  evals-mcp:
    build: ./packages/mcp-servers/evals-mcp
    ports:
      - "8084:8084"
    environment:
      PYTHONUNBUFFERED: "1"
      EVALS_MCP_RAGAS_THRESHOLD: "0.75"
      EVALS_MCP_FACTSCORE_THRESHOLD: "0.7"
      EVALS_MCP_TRUTHFULQA_THRESHOLD: "0.65"
    develop:
      watch:
        - path: ./packages/mcp-servers/evals-mcp
          action: sync
          target: /app

  compression-mcp:
    build: ./packages/mcp-servers/compression-mcp
    ports:
      - "8085:8085"
    environment:
      PYTHONUNBUFFERED: "1"
      COMPRESSION_MCP_ENABLE_LLMLINGUA: "0"
    develop:
      watch:
        - path: ./packages/mcp-servers/compression-mcp
          action: sync
          target: /app

  expertise-mcp:
    build: ./packages/mcp-servers/expertise-mcp
    ports:
      - "8086:8080"  # Use port 8086 externally to avoid conflicts
    environment:
      PYTHONUNBUFFERED: "1"
      EXPERTISE_MCP_LOG_LEVEL: "info"
    volumes:
      - ./configs:/app/configs:ro  # Mount configs directory for doctrines
    develop:
      watch:
        - path: ./packages/mcp-servers/expertise-mcp
          action: sync
          target: /app

  postgres:
    image: postgres:17
    environment:
      # checkov:skip=CKV_SECRET_4: Dev-only default password
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: stratmaster
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  opensearch:
    image: opensearchproject/opensearch:3.2.0
    environment:
      discovery.type: single-node
      plugins.security.disabled: "true"
      OPENSEARCH_JAVA_OPTS: "-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    volumes:
      - opensearch_data:/usr/share/opensearch/data

  minio:
    image: quay.io/minio/minio:latest
    command: server /data --console-address :9001
    environment:
      MINIO_ROOT_USER: stratmaster
      # checkov:skip=CKV_SECRET_4: Dev-only default password
      MINIO_ROOT_PASSWORD: stratmaster123
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  searxng:
    image: searxng/searxng:latest
    environment:
      SEARXNG_HOSTNAME: http://searxng:8080
    ports:
      - "8100:8080"

  temporal:
    image: temporalio/auto-setup:1.28.1
    environment:
      DB_PORT: 5432
      DB_HOST: postgres
      DB_NAME: temporal
      DB_USER: postgres
      # checkov:skip=CKV_SECRET_4: Dev-only default password
      DB_PWD: postgres
      DYNAMIC_CONFIG_FILE_PATH: config/dynamicconfig/development-sql.yaml
      VISIBILITY_DB_NAME: temporal_visibility
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "7233:7233"

  temporal-ui:
    image: temporalio/web:1.21.0
    environment:
      TEMPORAL_ADDRESS: temporal:7233
    depends_on:
      temporal:
        condition: service_started
    ports:
      - "8088:8080"

  langfuse:
    image: langfuse/langfuse:latest
    environment:
      # checkov:skip=CKV_SECRET_4: Dev-only embedded credentials for local Postgres
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/langfuse
      # checkov:skip=CKV_SECRET_6: Dev-only default secret; not used in production
      NEXTAUTH_SECRET: insecure_dev_secret
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "3000:3000"

  otel-collector:
    image: otel/opentelemetry-collector:0.136.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./configs/otel/collector.yaml:/etc/otel-collector-config.yaml:ro
    ports:
      - "4317:4317"
      - "4318:4318"

  keycloak:
    image: quay.io/keycloak/keycloak:26.3
    command: start-dev --import-realm
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    ports:
      - "8089:8080"

  redis:
    image: redis:8-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  nebula:
    image: vesoft/nebula-standalone:latest
    environment:
      USER: root
    ports:
      - "9669:9669"
      - "19669:19669"
      - "29669:29669"
    volumes:
      - nebula_data:/data

  vllm:
    image: vllm/vllm-openai:latest
    environment:
      VLLM_CPU: "1"
      VLLM_MODEL: facebook/opt-1.3b
    ports:
      - "8000:8000"

  playwright-worker:
    image: mcr.microsoft.com/playwright/python:v1.55.0-jammy
    profiles: [workers]
    command: sleep infinity

  # Phase 2 Implementation - Production Telemetry Dashboard
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./configs/telemetry/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"  # Avoid conflict with Langfuse
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: false
      GF_INSTALL_PLUGINS: grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/telemetry/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
      - ./configs/telemetry/grafana-dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml:ro
      - ./configs/telemetry/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus

  # Constitutional ML Services
  constitutional-bert:
    image: huggingface/transformers-pytorch-gpu:latest
    profiles: [ml]  # Optional ML services
    environment:
      TRANSFORMERS_CACHE: /cache
      MODEL_NAME: constitutional-bert-v2
    volumes:
      - ml_models_cache:/cache
    ports:
      - "8085:8000"

  # Real-time Collaboration WebSocket Service
  collaboration-ws:
    build: ./packages/collaboration
    profiles: [collaboration]  # Optional collaboration features
    ports:
      - "8084:8084"
    environment:
      PYTHONUNBUFFERED: "1"
      REDIS_URL: redis://redis:6379/1
      WEBSOCKET_PORT: 8084
    depends_on:
      redis:
        condition: service_healthy

volumes:
  postgres_data:
  qdrant_data:
  opensearch_data:
  minio_data:
  research_cache:
  nebula_data:
  redis_data:
  prometheus_data:
  grafana_data:
  ml_models_cache:
